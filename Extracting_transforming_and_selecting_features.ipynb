{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHpLGCqa1hqj",
        "outputId": "92a62cc2-c69a-478e-8717-16879538113d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler, ChiSqSelector\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col\n"
      ],
      "metadata": {
        "id": "L9Nya43jCCb-"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"BostonHousingFeatureEngineeringDone\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "HEMFBY6kD8d5"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the dataset into PySpark\n",
        "# Assume we have a CSV file with headers\n",
        "data = spark.read.csv(\"/content/HousingData.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Print the schema to see data types\n",
        "data.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1pnGdIHECQA",
        "outputId": "6df9a68f-6118-4b71-feec-8137f9272aa0"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CRIM: string (nullable = true)\n",
            " |-- ZN: string (nullable = true)\n",
            " |-- INDUS: string (nullable = true)\n",
            " |-- CHAS: string (nullable = true)\n",
            " |-- NOX: double (nullable = true)\n",
            " |-- RM: double (nullable = true)\n",
            " |-- AGE: string (nullable = true)\n",
            " |-- DIS: double (nullable = true)\n",
            " |-- RAD: integer (nullable = true)\n",
            " |-- TAX: integer (nullable = true)\n",
            " |-- PTRATIO: double (nullable = true)\n",
            " |-- B: double (nullable = true)\n",
            " |-- LSTAT: string (nullable = true)\n",
            " |-- MEDV: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical columns\n",
        "categorical_cols = [\"CHAS\", \"RAD\"]\n",
        "numerical_cols = [\"CRIM\", \"ZN\", \"INDUS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]"
      ],
      "metadata": {
        "id": "pjLrxcjcEFmD"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import DoubleType\n",
        "from pyspark.sql.types import DoubleType"
      ],
      "metadata": {
        "id": "DjrzkAJbEV5g"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Handle missing values and convert to appropriate types\n",
        "for col_name in numerical_cols + [\"MEDV\"]:\n",
        "    data = data.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
        "\n",
        "# Use Imputer to handle NaN values in numerical columns\n",
        "imputer = Imputer(inputCols=numerical_cols, outputCols=numerical_cols)\n",
        "imputer.setStrategy(\"mean\")  # You can change this to \"median\" or a constant value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbv1XZ7nGFOM",
        "outputId": "2b441a5d-1e5c-4e01-9275-a31542f07f8c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Imputer_867407f8374e"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Handle categorical features\n",
        "# For categorical columns, replace NaN with a placeholder value before indexing\n",
        "from pyspark.sql.functions import when, col # Import the when function\n",
        "for cat_col in categorical_cols:\n",
        "    data = data.withColumn(cat_col, when(col(cat_col).isNull(), \"Unknown\").otherwise(col(cat_col)))\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "encoders = [OneHotEncoder(inputCol=f\"{col}_indexed\", outputCol=f\"{col}_encoded\") for col in categorical_cols]"
      ],
      "metadata": {
        "id": "ULZZcXuLGS2k"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Scale numerical features\n",
        "assembler_num = VectorAssembler(inputCols=numerical_cols, outputCol=\"num_features\")\n",
        "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"scaled_num_features\", withStd=True, withMean=True)\n"
      ],
      "metadata": {
        "id": "3gFDDjiAGVuF"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Combine features\n",
        "encoded_cols = [f\"{col}_encoded\" for col in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=encoded_cols + [\"scaled_num_features\"], outputCol=\"features\")\n"
      ],
      "metadata": {
        "id": "3eBNV1KpGfch"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Perform feature selection\n",
        "median_medv = data.approxQuantile(\"MEDV\", [0.5], 0.01)[0]\n",
        "data = data.withColumn(\"label\", (col(\"MEDV\") > median_medv).cast(\"double\"))\n",
        "\n",
        "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"label\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iZB-tAaAGh2-"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the pipeline\n",
        "pipeline = Pipeline(stages=[imputer] + indexers + encoders + [assembler_num, scaler, assembler, selector])\n",
        "\n",
        "try:\n",
        "    model = pipeline.fit(data)\n",
        "    result = model.transform(data)\n",
        "\n",
        "    # Show the result\n",
        "    result.select(\"selected_features\", \"label\").show(5, truncate=False)\n",
        "\n",
        "    # Get feature importances\n",
        "    feature_importances = model.stages[-1].selectedFeatures\n",
        "    print(\"Selected feature indices:\", feature_importances)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    # Optionally, you can add more detailed error handling here\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhgrmRYBGj8U",
        "outputId": "4ef2d216-0784-42ef-85fb-41201f70e7d2"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|selected_features                                                                                                          |label|\n",
            "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|(10,[5,6,7,8,9],[0.29614984653077786,-1.3097142836688263,-0.14407485473245793,-0.6659491794887344,-1.4575579672895913])    |1.0  |\n",
            "|(10,[5,6,7,8,9],[-0.4891544449063988,-0.5991779446787058,-0.7395303607434325,-0.9863533804386955,-0.3027944997494501])     |1.0  |\n",
            "|(10,[5,6,7,8,9],[-0.4891544449063988,-0.5991779446787058,-0.7395303607434325,-0.9863533804386955,-0.3027944997494501])     |1.0  |\n",
            "|(10,[3,5,6,7,8,9],[1.0,-0.4891544449063988,-1.3291196878849432,-0.8344580501075004,-1.105021603012755,0.11292034856500006])|1.0  |\n",
            "|(10,[3,5,6,7,8,9],[1.0,-0.4891544449063988,-1.3291196878849432,-0.8344580501075004,-1.105021603012755,0.11292034856500006])|1.0  |\n",
            "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Selected feature indices: [1, 3, 4, 6, 9, 13, 14, 15, 19, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "ZaJzf3s5Glsn"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiaNjKe4GoUU"
      },
      "execution_count": 112,
      "outputs": []
    }
  ]
}